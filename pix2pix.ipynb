{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wcSM2qnZSl5O",
    "outputId": "b9a0b034-4adb-4d61-d4cf-7b3d02a0b9f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(boto3.__version__)\n",
    "# !pip install boto3>=1.5.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-051760164721\n",
      "arn:aws:iam::051760164721:role/service-role/AmazonSageMaker-ExecutionRole-20221123T181534\n",
      "us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker, boto3, os\n",
    "\n",
    "default_bucket = sagemaker.Session().default_bucket()\n",
    "region = sagemaker.Session().boto_region_name\n",
    "role   = sagemaker.get_execution_role()\n",
    "print(default_bucket)\n",
    "print(role)\n",
    "print(region)\n",
    "# boto3.Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #HOW TO READ IMAGES FROM AN S3 BUCKET TO A NOTEBOOK\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.image as mpimg\n",
    "# import boto3\n",
    "# import tempfile\n",
    "\n",
    "# bucket_name = 'guatemala.tns'\n",
    "# s3 = boto3.resource('s3', region_name=region)\n",
    "# bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "\n",
    "# for key in bucket.objects.all():\n",
    "#     if 'val' in key.key:\n",
    "#         object = bucket.Object(key.key)\n",
    "#         tmp = tempfile.NamedTemporaryFile()\n",
    "\n",
    "#         with open(tmp.name, 'wb') as f:\n",
    "#             object.download_fileobj(f)\n",
    "#             img=mpimg.imread(tmp.name)\n",
    "#             plt.imshow(img)\n",
    "#             plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"Guatemala\"\n",
    "bucket = 'guatemala.tns'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the data has been written to s3\n",
    "# ! aws s3 ls {bucket}/{prefix}/data --recursive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Output path for the trained model\n",
    "# model_path = f's3://{bucket}/{prefix}/models'\n",
    "# print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read data from s3\n",
    "# from sagemaker.session import TrainingInput\n",
    "# data.zip = TrainingInput(f's3://{bucket}/{prefix}/data/Guatemala.zip', content_type='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip ./Guatemala-20221103T003024Z-001.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3D_pGf5XWE0P"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as v1\n",
    "\n",
    "from numpy import load\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import boto3\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip ./Guatemala-20221103T003024Z-001.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "00ZlBWyrXp9W"
   },
   "outputs": [],
   "source": [
    "BASE_PATH = \"Guatemala\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "2eEKX_qsTYh7"
   },
   "outputs": [],
   "source": [
    "# # load all images in a directory into memory\n",
    "# def load_images(path, size=(256,512)):\n",
    "# \tsrc_list, tar_list = list(), list()\n",
    "# \t# enumerate filenames in directory, assume all are images\n",
    "# \tfor filename in listdir(path):\n",
    "# \t\t# load and resize the image\n",
    "# \t\tpixels = load_img(path + filename, target_size=size)\n",
    "# \t\t# convert to numpy array\n",
    "# \t\tpixels = img_to_array(pixels)\n",
    "# \t\t# split into satellite and map\n",
    "# \t\tsat_img, map_img = pixels[:, :512], pixels[:, 512:]\n",
    "# \t\tsrc_list.append(sat_img)\n",
    "# \t\ttar_list.append(map_img)\n",
    "# \treturn [asarray(src_list), asarray(tar_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yfTLKd6NTmmj",
    "outputId": "052aef6a-1e81-41ff-efd0-31cb7629f9a5"
   },
   "outputs": [],
   "source": [
    "# # load, split and scale the maps dataset ready for training\n",
    "# from os import listdir\n",
    "# from numpy import asarray\n",
    "# from numpy import vstack\n",
    "# from tensorflow.keras.preprocessing.image import img_to_array\n",
    "# from tensorflow.keras.preprocessing.image import load_img\n",
    "# from numpy import savez_compressed\n",
    " \n",
    "# # load all images in a directory into memory\n",
    "# def load_images(path, size=(512,1024)):\n",
    "# \tsrc_list, tar_list = list(), list()\n",
    "# \t# enumerate filenames in directory, assume all are images\n",
    "# \tfor filename in listdir(path):\n",
    "# \t\t# load and resize the image\n",
    "# \t\tpixels = load_img(path + filename, target_size=size)\n",
    "# \t\t# convert to numpy array\n",
    "# \t\tpixels = img_to_array(pixels)\n",
    "# \t\t# split into satellite and map\n",
    "# \t\tsat_img, map_img = pixels[:, :512], pixels[:, 512:]\n",
    "# \t\tsrc_list.append(sat_img)\n",
    "# \t\ttar_list.append(map_img)\n",
    "# \treturn [asarray(src_list), asarray(tar_list)]\n",
    " \n",
    "# # dataset path\n",
    "# path = BASE_PATH + '/train/'\n",
    "# # load dataset\n",
    "# [src_images, tar_images] = load_images(path)\n",
    "# print('Loaded: ', src_images.shape, tar_images.shape)\n",
    "# # save as compressed numpy array\n",
    "# filename = BASE_PATH + 'cherries_train.npz'\n",
    "# savez_compressed(filename, src_images, tar_images)\n",
    "# print('Saved dataset: ', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded:  (0,) (0,)\n",
      "Saved dataset:  Guatemalacherries_train.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# load, split and scale the maps dataset ready for training\n",
    "from os import listdir\n",
    "from numpy import asarray\n",
    "from numpy import vstack\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from numpy import savez_compressed\n",
    "\n",
    "region = sagemaker.Session().boto_region_name\n",
    "s3 = boto3.resource('s3', region_name=region)\n",
    "theBucket = s3.Bucket('guatemala.tns')\n",
    "\n",
    "# load all images in a directory into memory\n",
    "def load_images(folder, size=(512,1024)):\n",
    "    src_list, tar_list = list(), list()\n",
    "    # enumerate filenames in directory, assume all are images\n",
    "    for key in theBucket.objects.all():\n",
    "        if folder in key.key:\n",
    "            object = theBucket.Object(key.key)\n",
    "            tmp = tempfile.NamedTemporaryFile()\n",
    "\n",
    "            with open(tmp.name, 'wb') as f:\n",
    "                object.download_fileobj(f)\n",
    "                img=mpimg.imread(tmp.name)\n",
    "\n",
    "        # load and resize the image\n",
    "        # convert to numpy array\n",
    "            pixels = img_to_array(img)\n",
    "            # split into satellite and map\n",
    "            sat_img, map_img = pixels[:, :512], pixels[:, 512:]\n",
    "            src_list.append(sat_img)\n",
    "            tar_list.append(map_img)\n",
    "    return [asarray(src_list), asarray(tar_list)]\n",
    " \n",
    "# dataset path\n",
    "# path = BASE_PATH + '/train/'\n",
    "# load dataset\n",
    "[src_images, tar_images] = load_images('train')\n",
    "print('Loaded: ', src_images.shape, tar_images.shape)\n",
    "# save as compressed numpy array\n",
    "filename = BASE_PATH + 'cherries_train.npz'\n",
    "savez_compressed(filename, src_images, tar_images)\n",
    "print('Saved dataset: ', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "Cjxueo-dUT_O",
    "outputId": "6315d310-6521-41f2-c43b-c6c5af3fc63d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded:  (0,) (0,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c9e9dbf64ad1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# plot target image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABxCAYAAAANvCfuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAABHUlEQVR4nO3RwQkAIBDAMHX/nc8hfEghmaDQPTOLrvM7gDcGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2CcgXEGxhkYZ2DcBaQdA9++I3rJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the prepared dataset\n",
    "from numpy import load\n",
    "from matplotlib import pyplot\n",
    "# load the dataset\n",
    "data = load(filename)\n",
    "src_images, tar_images = data['arr_0'], data['arr_1']\n",
    "print('Loaded: ', src_images.shape, tar_images.shape)\n",
    "# plot source images\n",
    "n_samples = 3\n",
    "for i in range(n_samples):\n",
    "\tpyplot.subplot(2, n_samples, 1 + i)\n",
    "\tpyplot.axis('off')\n",
    "\tpyplot.imshow(src_images[i].astype('uint8'))\n",
    "# plot target image\n",
    "for i in range(n_samples):\n",
    "\tpyplot.subplot(2, n_samples, 1 + n_samples + i)\n",
    "\tpyplot.axis('off')\n",
    "\tpyplot.imshow(tar_images[i].astype('uint8'))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5Ot6GFlUhz4"
   },
   "outputs": [],
   "source": [
    "# define the discriminator model\n",
    "def define_discriminator(image_shape):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# source image input\n",
    "\tin_src_image = Input(shape=image_shape)\n",
    "\t# target image input\n",
    "\tin_target_image = Input(shape=image_shape)\n",
    "\t# concatenate images channel-wise\n",
    "\tmerged = Concatenate()([in_src_image, in_target_image])\n",
    "\t# C64\n",
    "\td = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# C128\n",
    "\td = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# C256\n",
    "\td = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# C512\n",
    "\td = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# second last output layer\n",
    "\td = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# patch output\n",
    "\td = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "\tpatch_out = Activation('sigmoid')(d)\n",
    "\t# define model\n",
    "\tmodel = Model([in_src_image, in_target_image], patch_out)\n",
    "\t# compile model\n",
    "\topt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKClAi9mVH3e"
   },
   "outputs": [],
   "source": [
    "# define an encoder block\n",
    "def define_encoder_block(layer_in, n_filters, batchnorm=True, training=True):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# add downsampling layer\n",
    "\tg = Conv2D(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "\t# conditionally add batch normalization\n",
    "\tif batchnorm:\n",
    "\t\tg = BatchNormalization()(g, training=training)\n",
    "\t# leaky relu activation\n",
    "\tg = LeakyReLU(alpha=0.2)(g)\n",
    "\treturn g\n",
    " \n",
    "# define a decoder block\n",
    "def decoder_block(layer_in, skip_in, n_filters, dropout=True, training=True):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# add upsampling layer\n",
    "\tg = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "\t# add batch normalization\n",
    "\tg = BatchNormalization()(g, training=training)\n",
    "\t# conditionally add dropout\n",
    "\tif dropout:\n",
    "\t\tg = Dropout(0.5)(g, training=training)\n",
    "\t# merge with skip connection\n",
    "\tg = Concatenate()([g, skip_in])\n",
    "\t# relu activation\n",
    "\tg = Activation('relu')(g)\n",
    "\treturn g\n",
    " \n",
    "# define the standalone generator model\n",
    "def define_generator(image_shape=(512,512,3), training=True):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# image input\n",
    "\tin_image = Input(shape=image_shape)\n",
    "\t# encoder model\n",
    "\te1 = define_encoder_block(in_image, 64, batchnorm=False, training=training)\n",
    "\te2 = define_encoder_block(e1, 128, training=training)\n",
    "\te3 = define_encoder_block(e2, 256, training=training)\n",
    "\te4 = define_encoder_block(e3, 512, training=training)\n",
    "\te5 = define_encoder_block(e4, 512, training=training)\n",
    "\te6 = define_encoder_block(e5, 512, training=training)\n",
    "\te7 = define_encoder_block(e6, 512, training=training)\n",
    "\t# bottleneck, no batch norm and relu\n",
    "\tb = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7)\n",
    "\tb = Activation('relu')(b)\n",
    "\t# decoder model\n",
    "\td1 = decoder_block(b, e7, 512, training=training)\n",
    "\td2 = decoder_block(d1, e6, 512, training=training)\n",
    "\td3 = decoder_block(d2, e5, 512, training=training)\n",
    "\td4 = decoder_block(d3, e4, 512, dropout=False, training=training)\n",
    "\td5 = decoder_block(d4, e3, 256, dropout=False, training=training)\n",
    "\td6 = decoder_block(d5, e2, 128, dropout=False, training=training)\n",
    "\td7 = decoder_block(d6, e1, 64, dropout=False, training=training)\n",
    "\t# output\n",
    "\tg = Conv2DTranspose(3, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7)\n",
    "\tout_image = Activation('tanh')(g)\n",
    "\t# define model\n",
    "\tmodel = Model(in_image, out_image)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMNnGtKoVTo9"
   },
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model, image_shape):\n",
    "\t# make weights in the discriminator not trainable\n",
    "\tfor layer in d_model.layers:\n",
    "\t\tif not isinstance(layer, BatchNormalization):\n",
    "\t\t\tlayer.trainable = False\n",
    "\t# define the source image\n",
    "\tin_src = Input(shape=image_shape)\n",
    "\t# connect the source image to the generator input\n",
    "\tgen_out = g_model(in_src)\n",
    "\t# connect the source input and generator output to the discriminator input\n",
    "\tdis_out = d_model([in_src, gen_out])\n",
    "\t# src image as input, generated image and classification output\n",
    "\tmodel = Model(in_src, [dis_out, gen_out])\n",
    "\t# compile model\n",
    "\topt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\tmodel.compile(loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pf3SDOjeVXFv"
   },
   "outputs": [],
   "source": [
    "# load and prepare training images\n",
    "def load_real_samples(filename):\n",
    "\t# load compressed arrays\n",
    "\tdata = load(filename)\n",
    "\t# unpack arrays\n",
    "\tX1, X2 = data['arr_0'], data['arr_1']\n",
    "\t# scale from [0,255] to [-1,1]\n",
    "\tX1 = (X1 - 127.5) / 127.5\n",
    "\tX2 = (X2 - 127.5) / 127.5\n",
    "\treturn [X1, X2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGqip-RjVfc1"
   },
   "outputs": [],
   "source": [
    "# select a batch of random samples, returns images and target\n",
    "def generate_real_samples(dataset, n_samples, patch_shape):\n",
    "\t# unpack dataset\n",
    "\ttrainA, trainB = dataset\n",
    "\t# choose random instances\n",
    "\tix = randint(0, trainA.shape[0], n_samples)\n",
    "\t# retrieve selected images\n",
    "\tX1, X2 = trainA[ix], trainB[ix]\n",
    "\t# generate 'real' class labels (1)\n",
    "\ty = ones((n_samples, patch_shape, patch_shape, 1))\n",
    "\treturn [X1, X2], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MP4iY4yLVk8v"
   },
   "outputs": [],
   "source": [
    "# generate a batch of images, returns images and targets\n",
    "def generate_fake_samples(g_model, samples, patch_shape):\n",
    "\t# generate fake instance\n",
    "\tX = g_model.predict(samples)\n",
    "\t# create 'fake' class labels (0)\n",
    "\ty = zeros((len(X), patch_shape, patch_shape, 1))\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOkzbDQ1VnJn"
   },
   "outputs": [],
   "source": [
    "# generate samples and save as a plot and save the model\n",
    "def summarize_performance(step, g_model, dataset, n_samples=3):\n",
    "  # select a sample of input images\n",
    "  [X_realA, X_realB], _ = generate_real_samples(dataset, n_samples, 1)\n",
    "  # generate a batch of fake samples\n",
    "  X_fakeB, _ = generate_fake_samples(g_model, X_realA, 1)\n",
    "  # scale all pixels from [-1,1] to [0,1]\n",
    "  X_realA = (X_realA + 1) / 2.0\n",
    "  X_realB = (X_realB + 1) / 2.0\n",
    "  X_fakeB = (X_fakeB + 1) / 2.0\n",
    "  # plot real source images\n",
    "  for i in range(n_samples):\n",
    "    pyplot.subplot(3, n_samples, 1 + i)\n",
    "    pyplot.axis('off')\n",
    "    pyplot.imshow(X_realA[i])\n",
    "  # plot generated target image\n",
    "  for i in range(n_samples):\n",
    "    pyplot.subplot(3, n_samples, 1 + n_samples + i)\n",
    "    pyplot.axis('off')\n",
    "    pyplot.imshow(X_fakeB[i])\n",
    "  # plot real target image\n",
    "  for i in range(n_samples):\n",
    "    pyplot.subplot(3, n_samples, 1 + n_samples*2 + i)\n",
    "    pyplot.axis('off')\n",
    "    pyplot.imshow(X_realB[i])\n",
    "  # save plot to file\n",
    "  filename1 = BASE_PATH + 'plot_%06d.png' % (step+1)\n",
    "  pyplot.savefig(filename1)\n",
    "\n",
    "# Save plots to s3\n",
    "\n",
    "  boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'plots/'+str(filename1))).upload_file(Filename=filename1)  \n",
    "\n",
    "  pyplot.close()\n",
    "  # save the generator model\n",
    "  filename2 = BASE_PATH + 'model_%06d.h5' % (step+1)\n",
    "  g_model.save(filename2)\n",
    "\n",
    " # Save models to s3                                                   \n",
    "  boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'models/'+str(filename2))).upload_file(Filename=filename2)\n",
    "  \n",
    "  weights = 'model_%06d.weight' % (step+1)                                                   \n",
    "  g_model.save_weights(weights)\n",
    "                                                \n",
    " # Save weights to s3                                              \n",
    "  boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'weights/'+str(weights))).upload_file(Filename=weights + '.index')\n",
    "  print('>Saved: %s and %s' % (filename1, filename2))\n",
    "  print(f'weights path: {weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2z1AjhwaVrCQ"
   },
   "outputs": [],
   "source": [
    "def train(d_model, g_model, gan_model, dataset, n_epochs=100, n_batch=1):\n",
    "\t# determine the output square shape of the discriminator\n",
    "\tn_patch = d_model.output_shape[1]\n",
    "\t# unpack dataset\n",
    "\ttrainA, trainB = dataset\n",
    "\t# calculate the number of batches per training epoch\n",
    "\tbat_per_epo = int(len(trainA) / n_batch)\n",
    "\t# calculate the number of training iterations\n",
    "\tn_steps = bat_per_epo * n_epochs\n",
    "\t# manually enumerate epochs\n",
    "\tfor i in range(n_steps):\n",
    "\t\t# select a batch of real samples\n",
    "\t\t[X_realA, X_realB], y_real = generate_real_samples(dataset, n_batch, n_patch)\n",
    "\t\t# generate a batch of fake samples\n",
    "\t\tX_fakeB, y_fake = generate_fake_samples(g_model, X_realA, n_patch)\n",
    "\t\t# update discriminator for real samples\n",
    "\t\td_loss1 = d_model.train_on_batch([X_realA, X_realB], y_real)\n",
    "\t\t# update discriminator for generated samples\n",
    "\t\td_loss2 = d_model.train_on_batch([X_realA, X_fakeB], y_fake)\n",
    "\t\t# update the generator\n",
    "\t\tg_loss, _, _ = gan_model.train_on_batch(X_realA, [y_real, X_realB])\n",
    "\t\t# summarize performance\n",
    "\t\tprint('>%d, d1[%.3f] d2[%.3f] g[%.3f]' % (i+1, d_loss1, d_loss2, g_loss))\n",
    "\t\t# summarize model performance\n",
    "\t\tif (i+1) % (bat_per_epo * 10) == 0:\n",
    "\t\t\tsummarize_performance(i, g_model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qoq3KSZXVvtY",
    "outputId": "166f6631-78ca-4f8d-8c15-c1650f6b20e0"
   },
   "outputs": [],
   "source": [
    "# load image data\n",
    "dataset = load_real_samples(filename)\n",
    "print('Loaded', dataset[0].shape, dataset[1].shape)\n",
    "# define input shape based on the loaded dataset\n",
    "image_shape = dataset[0].shape[1:]\n",
    "# define the models\n",
    "d_model = define_discriminator(image_shape)\n",
    "g_model = define_generator(image_shape)\n",
    "# define the composite model\n",
    "gan_model = define_gan(g_model, d_model, image_shape)\n",
    "# train model\n",
    "train(d_model, g_model, gan_model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FcEfsqnZkaf"
   },
   "outputs": [],
   "source": [
    "# example of loading a pix2pix model and using it for image to image translation\n",
    "from tensorflow.keras.models import load_model\n",
    "from numpy import load\n",
    "from numpy import vstack\n",
    "from matplotlib import pyplot\n",
    "from numpy.random import randint\n",
    " \n",
    "# load and prepare training images\n",
    "def load_real_samples(filename):\n",
    "\t# load compressed arrays\n",
    "\tdata = load(filename)\n",
    "\t# unpack arrays\n",
    "\tX1, X2 = data['arr_0'], data['arr_1']\n",
    "\t# scale from [0,255] to [-1,1]\n",
    "\tX1 = (X1 - 127.5) / 127.5\n",
    "\tX2 = (X2 - 127.5) / 127.5\n",
    "\treturn [X1, X2]\n",
    " \n",
    "# plot source, generated and target images\n",
    "def plot_images(src_img, gen_img, tar_img):\n",
    "\timages = vstack((src_img, gen_img, tar_img))\n",
    "\t# scale from [-1,1] to [0,1]\n",
    "\timages = (images + 1) / 2.0\n",
    "\ttitles = ['Source', 'Generated', 'Expected']\n",
    "\t# plot images row by row\n",
    "\tfor i in range(len(images)):\n",
    "\t\t# define subplot\n",
    "\t\tpyplot.subplot(1, 3, 1 + i)\n",
    "\t\t# turn off axis\n",
    "\t\tpyplot.axis('off')\n",
    "\t\t# plot raw pixel data\n",
    "\t\tpyplot.imshow(images[i])\n",
    "\t\t# show title\n",
    "\t\tpyplot.title(titles[i])\n",
    "\tpyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u24wkkwKX11M",
    "outputId": "839bf595-5c00-44d4-9bb1-4fe43afd8ba2"
   },
   "outputs": [],
   "source": [
    "# dataset path\n",
    "# load dataset\n",
    "[src_images, tar_images] = load_images('val')\n",
    "print('Loaded: ', src_images.shape, tar_images.shape)\n",
    "# save as compressed numpy array\n",
    "filename = BASE_PATH + 'cherries_val.npz'\n",
    "savez_compressed(filename, src_images, tar_images)\n",
    "print('Saved dataset: ', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "id": "QshL7NWgZ3Ib",
    "outputId": "cda21f1d-3fb6-4552-f586-1bd7f6f14a73"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "[X1, X2] = load_real_samples(filename)\n",
    "print('Loaded', X1.shape, X2.shape)\n",
    "# load model\n",
    "# model = load_model(BASE_PATH + 'model_001060.h5')\n",
    "model = load_model(BASE_PATH + 'model_000550.h5')\n",
    "# select random example\n",
    "ix = randint(0, len(X1), 1)\n",
    "src_image, tar_image = X1[ix], X2[ix]\n",
    "# generate image from source\n",
    "gen_image = model.predict(src_image)\n",
    "# plot all three images\n",
    "plot_images(src_image, gen_image, tar_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sfW8LxakkzpG",
    "outputId": "89b71e53-deed-426f-e505-491f4f0598e1"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = BASE_PATH + \"checkpoints/model.ckpt\"\n",
    "model = load_model(BASE_PATH + 'model_005300.h5')\n",
    "\n",
    "model.save_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "773TL4q3aHaI",
    "outputId": "6b140970-eade-45ae-c4d9-bf4e4378daa9"
   },
   "outputs": [],
   "source": [
    "eval_model = define_generator(image_shape=(512, 512, 3), training=False)\n",
    "eval_model.load_weights(BASE_PATH + \"checkpoints/model.ckpt\")\n",
    "# eval_model.summary()\n",
    "\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(eval_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open(BASE_PATH + 'model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)\n",
    "\n",
    "# Save tf model to s3\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'models/tflite/'+str(BASE_PATH) + 'model.tflite').upload_file(Filename=weights)                                                    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5XRGIe-WlGlo",
    "outputId": "39fb6524-25cf-4aeb-86e3-41cb407de881"
   },
   "outputs": [],
   "source": [
    "# Load the TFLite model and allocate tensors.\n",
    "import numpy as np\n",
    "interpreter = tf.lite.Interpreter(model_path=BASE_PATH + \"model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array([X1[1]], dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "vuGLm5mNcB7a",
    "outputId": "82d4d3a4-bd9b-40e1-cb28-881f4facd454"
   },
   "outputs": [],
   "source": [
    "pyplot.imshow(output_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "z_1q4w3QlkP8",
    "outputId": "1d66bae6-9642-4cfc-c8d8-0bf9f1550434"
   },
   "outputs": [],
   "source": [
    "pyplot.imshow(X1[1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pix2pix.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
